# import csv
# import psycopg2
# from psycopg2.extras import execute_batch
# from config.config import Config

# def create_table(conn):
#     """T·∫°o b·∫£ng n·∫øu ch∆∞a t·ªìn t·∫°i v·ªõi c·ªôt geometry"""
#     cursor = conn.cursor()
    
#     # Ki·ªÉm tra v√† t·∫°o PostGIS extension
#     cursor.execute("""
#         CREATE EXTENSION IF NOT EXISTS postgis;
#     """)
    
#     # T·∫°o b·∫£ng v·ªõi geometry point
#     cursor.execute("""
#         CREATE TABLE IF NOT EXISTS poi_locations_uuid (
#             id UUID PRIMARY KEY,
#             name TEXT,
#             address TEXT,
#             lat DOUBLE PRECISION NOT NULL,
#             long DOUBLE PRECISION NOT NULL,
#             geom GEOMETRY(Point, 4326),
#             poi_type TEXT,
#             avg_star DOUBLE PRECISION,
#             total_reviews DOUBLE PRECISION,
#             normalize_stars_reviews DOUBLE PRECISION
#         );
#     """)
    
#     # T·∫°o spatial index cho c·ªôt geometry
#     cursor.execute("""
#         CREATE INDEX IF NOT EXISTS poi_locations_geom_idx 
#         ON poi_locations_uuid USING GIST(geom);
#     """)
    
#     # Th√™m c√°c c·ªôt m·ªõi n·∫øu ch∆∞a t·ªìn t·∫°i
#     cursor.execute("""
#         DO $$ 
#         BEGIN
#             -- Th√™m c·ªôt avg_star n·∫øu ch∆∞a c√≥
#             IF NOT EXISTS (SELECT 1 FROM information_schema.columns 
#                           WHERE table_name='poi_locations_uuid' AND column_name='avg_star') THEN
#                 ALTER TABLE poi_locations_uuid ADD COLUMN avg_star DOUBLE PRECISION;
#             END IF;
            
#             -- Th√™m c·ªôt total_reviews n·∫øu ch∆∞a c√≥
#             IF NOT EXISTS (SELECT 1 FROM information_schema.columns 
#                           WHERE table_name='poi_locations_uuid' AND column_name='total_reviews') THEN
#                 ALTER TABLE poi_locations_uuid ADD COLUMN total_reviews DOUBLE PRECISION;
#             END IF;
            
#             -- Th√™m c·ªôt normalize_stars_reviews n·∫øu ch∆∞a c√≥
#             IF NOT EXISTS (SELECT 1 FROM information_schema.columns 
#                           WHERE table_name='poi_locations_uuid' AND column_name='normalize_stars_reviews') THEN
#                 ALTER TABLE poi_locations_uuid ADD COLUMN normalize_stars_reviews DOUBLE PRECISION;
#             END IF;
#         END $$;
#     """)
    
#     conn.commit()
#     cursor.close()
#     print("‚úì T·∫°o b·∫£ng v√† index th√†nh c√¥ng")
#     print("‚úì ƒê√£ th√™m/ki·ªÉm tra c√°c c·ªôt: avg_star, total_reviews, normalize_stars_reviews")

# def import_csv_to_postgres(csv_file_path, batch_size=100):
#     """
#     Import d·ªØ li·ªáu t·ª´ CSV v√†o PostgreSQL v·ªõi geometry
    
#     Args:
#         csv_file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV
#         batch_size: S·ªë l∆∞·ª£ng records insert m·ªói l·∫ßn (default: 100)
#     """
#     # K·∫øt n·ªëi database
#     conn = psycopg2.connect(Config.get_db_connection_string())
    
#     try:
#         # T·∫°o b·∫£ng
#         create_table(conn)
        
#         # ƒê·ªçc v√† insert d·ªØ li·ªáu
#         cursor = conn.cursor()
        
#         with open(csv_file_path, 'r', encoding='utf-8-sig') as file:
#             csv_reader = csv.DictReader(file)
            
#             # Chu·∫©n b·ªã d·ªØ li·ªáu cho batch insert
#             batch_data = []
#             total_rows = 0
#             skipped_rows = 0
            
#             for row in csv_reader:
#                 try:
#                     # Validate d·ªØ li·ªáu
#                     if not row['id'] or not row['lat'] or not row['long']:
#                         skipped_rows += 1
#                         continue
                    
#                     lat = float(row['lat'])
#                     long = float(row['long'])
                    
#                     # Parse c√°c tr∆∞·ªùng ƒëi·ªÉm s·ªë (cho ph√©p gi√° tr·ªã r·ªóng)
#                     # ƒê·ªçc t·ª´ CSV v·ªõi t√™n c·ªôt g·ªëc: avg_score, final_score
#                     avg_star = float(row['avg_score']) if row.get('avg_score') and row['avg_score'].strip() else None
#                     total_reviews = float(row['total_reviews']) if row.get('total_reviews') and row['total_reviews'].strip() else None
#                     normalize_stars_reviews = float(row['final_score']) if row.get('final_score') and row['final_score'].strip() else None
                    
#                     # Th√™m v√†o batch
#                     batch_data.append((
#                         row['id'],
#                         row.get('name', ''),
#                         row.get('address', ''),
#                         lat,
#                         long,
#                         long,  # longitude
#                         lat,   # latitude
#                         row.get('poi_type', ''),
#                         avg_star,
#                         total_reviews,
#                         normalize_stars_reviews
#                     ))
                    
#                     # Insert batch khi ƒë·ªß k√≠ch th∆∞·ªõc
#                     if len(batch_data) >= batch_size:
#                         execute_batch(cursor, """
#                             INSERT INTO poi_locations_uuid (id, name, address, lat, long, geom, poi_type, avg_star, total_reviews, normalize_stars_reviews)
#                             VALUES (%s, %s, %s, %s, %s, ST_SetSRID(ST_MakePoint(%s, %s), 4326), %s, %s, %s, %s)
#                             ON CONFLICT (id) DO UPDATE SET
#                                 name = EXCLUDED.name,
#                                 address = EXCLUDED.address,
#                                 lat = EXCLUDED.lat,
#                                 long = EXCLUDED.long,
#                                 geom = EXCLUDED.geom,
#                                 poi_type = EXCLUDED.poi_type,
#                                 avg_star = EXCLUDED.avg_star,
#                                 total_reviews = EXCLUDED.total_reviews,
#                                 normalize_stars_reviews = EXCLUDED.normalize_stars_reviews;
#                         """, batch_data)
                        
#                         conn.commit()
#                         total_rows += len(batch_data)
#                         print(f"  ƒê√£ import {total_rows} records...")
#                         batch_data = []
                        
#                 except (ValueError, KeyError) as e:
#                     print(f"‚ö† L·ªói ·ªü d√≤ng: {row} - {e}")
#                     skipped_rows += 1
#                     continue
            
#             # Insert batch c√≤n l·∫°i
#             if batch_data:
#                 execute_batch(cursor, """
#                     INSERT INTO poi_locations (id, name, address, lat, long, geom, poi_type, avg_star, total_reviews, normalize_stars_reviews)
#                     VALUES (%s, %s, %s, %s, %s, ST_SetSRID(ST_MakePoint(%s, %s), 4326), %s, %s, %s, %s)
#                     ON CONFLICT (id) DO UPDATE SET
#                         name = EXCLUDED.name,
#                         address = EXCLUDED.address,
#                         lat = EXCLUDED.lat,
#                         long = EXCLUDED.long,
#                         geom = EXCLUDED.geom,
#                         poi_type = EXCLUDED.poi_type,
#                         avg_star = EXCLUDED.avg_star,
#                         total_reviews = EXCLUDED.total_reviews,
#                         normalize_stars_reviews = EXCLUDED.normalize_stars_reviews;
#                 """, batch_data)
                
#                 conn.commit()
#                 total_rows += len(batch_data)
            
#             cursor.close()
            
#             print(f"\n‚úì Ho√†n th√†nh!")
#             print(f"  - T·ªïng s·ªë records th√†nh c√¥ng: {total_rows}")
#             print(f"  - S·ªë records b·ªã b·ªè qua: {skipped_rows}")
            
#     except Exception as e:
#         print(f"‚úó L·ªói: {e}")
#         conn.rollback()
#         raise
    
#     finally:
#         conn.close()

# def verify_data(limit=5):
#     """Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ import"""
#     conn = psycopg2.connect(Config.get_db_connection_string())
#     cursor = conn.cursor()
    
#     # ƒê·∫øm t·ªïng s·ªë records
#     cursor.execute("SELECT COUNT(*) FROM poi_locations_uuid;")
#     count = cursor.fetchone()[0]
#     print(f"\nüìä Th·ªëng k√™:")
#     print(f"  - T·ªïng s·ªë POI trong database: {count}")
    
#     # Hi·ªÉn th·ªã m·ªôt v√†i records m·∫´u
#     cursor.execute(f"""
#         SELECT id, name, address, lat, long, poi_type, avg_star, total_reviews, normalize_stars_reviews, ST_AsText(geom) as geom_text
#         FROM poi_locations_uuid 
#         LIMIT {limit};
#     """)
    
#     print(f"\nüìù {limit} records m·∫´u:")
#     for row in cursor.fetchall():
#         name_display = row[1][:50] if row[1] else 'N/A'
#         print(f"  - {name_display}: ({row[3]}, {row[4]}) - {row[5]}")
#         print(f"    Address: {row[2][:50] if row[2] else 'N/A'}")
#         print(f"    Scores: avg={row[6]}, reviews={row[7]}, final={row[8]}")
#         print(f"    Geometry: {row[9]}")
    
#     cursor.close()
#     conn.close()

# if __name__ == "__main__":
#     # File CSV c·∫ßn import
#     csv_file = "viamo_full.csv"
    
#     print("üöÄ B·∫Øt ƒë·∫ßu import d·ªØ li·ªáu v√†o PostgreSQL...")
#     print(f"üìÅ File: {csv_file}")
#     print(f"üîó Database: {Config.DB_NAME} @ {Config.DB_HOST}\n")
    
#     # Import d·ªØ li·ªáu
#     import_csv_to_postgres(csv_file, batch_size=100)
    
#     # Ki·ªÉm tra k·∫øt qu·∫£
#     verify_data(limit=5)








import csv
import psycopg2
from psycopg2.extras import execute_batch
from config.config import Config


def create_table_if_not_exists(conn):
    """T·∫°o table poi_locations_uuid n·∫øu ch∆∞a t·ªìn t·∫°i"""
    cursor = conn.cursor()

    # Enable PostGIS
    cursor.execute("""
        CREATE EXTENSION IF NOT EXISTS postgis;
    """)

    # Create table
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS poi_locations_uuid (
            id UUID PRIMARY KEY,
            name TEXT,
            address TEXT,
            lat DOUBLE PRECISION NOT NULL,
            long DOUBLE PRECISION NOT NULL,
            geom GEOMETRY(Point, 4326),
            poi_type TEXT,
            avg_star DOUBLE PRECISION,
            total_reviews DOUBLE PRECISION,
            normalize_stars_reviews DOUBLE PRECISION
        );
    """)

    # Create spatial index
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS poi_locations_uuid_geom_idx
        ON poi_locations_uuid USING GIST (geom);
    """)

    conn.commit()
    cursor.close()
    print("‚úì Table poi_locations_uuid s·∫µn s√†ng")


def import_csv_to_postgres(csv_file_path, batch_size=100):
    """Import CSV v√†o PostgreSQL (upsert theo UUID)"""
    conn = psycopg2.connect(Config.get_db_connection_string())

    try:
        create_table_if_not_exists(conn)
        cursor = conn.cursor()

        with open(csv_file_path, "r", encoding="utf-8-sig") as file:
            reader = csv.DictReader(file)

            batch_data = []
            total_rows = 0
            skipped_rows = 0

            for row in reader:
                try:
                    if not row.get("id") or not row.get("lat") or not row.get("long"):
                        skipped_rows += 1
                        continue

                    lat = float(row["lat"])
                    long = float(row["long"])

                    avg_star = float(row["avg_score"]) if row.get("avg_score") else None
                    total_reviews = float(row["total_reviews"]) if row.get("total_reviews") else None
                    normalize_score = float(row["final_score"]) if row.get("final_score") else None

                    batch_data.append((
                        row["id"],
                        row.get("name"),
                        row.get("address"),
                        lat,
                        long,
                        long,  # longitude
                        lat,   # latitude
                        row.get("poi_type"),
                        avg_star,
                        total_reviews,
                        normalize_score
                    ))

                    if len(batch_data) >= batch_size:
                        execute_batch(cursor, UPSERT_SQL, batch_data)
                        conn.commit()
                        total_rows += len(batch_data)
                        batch_data.clear()
                        print(f"  ƒê√£ import {total_rows} records...")

                except Exception as e:
                    print(f"‚ö† B·ªè qua d√≤ng l·ªói: {e}")
                    skipped_rows += 1

            if batch_data:
                execute_batch(cursor, UPSERT_SQL, batch_data)
                conn.commit()
                total_rows += len(batch_data)

        cursor.close()
        print("\n‚úì IMPORT HO√ÄN T·∫§T")
        print(f"  - Th√†nh c√¥ng: {total_rows}")
        print(f"  - B·ªè qua: {skipped_rows}")

    except Exception as e:
        conn.rollback()
        print(f"‚úó L·ªói: {e}")
        raise
    finally:
        conn.close()


UPSERT_SQL = """
INSERT INTO poi_locations_uuid
(id, name, address, lat, long, geom, poi_type, avg_star, total_reviews, normalize_stars_reviews)
VALUES (
    %s, %s, %s, %s, %s,
    ST_SetSRID(ST_MakePoint(%s, %s), 4326),
    %s, %s, %s, %s
)
ON CONFLICT (id) DO UPDATE SET
    name = EXCLUDED.name,
    address = EXCLUDED.address,
    lat = EXCLUDED.lat,
    long = EXCLUDED.long,
    geom = EXCLUDED.geom,
    poi_type = EXCLUDED.poi_type,
    avg_star = EXCLUDED.avg_star,
    total_reviews = EXCLUDED.total_reviews,
    normalize_stars_reviews = EXCLUDED.normalize_stars_reviews;
"""


def verify_data(limit=5):
    """Ki·ªÉm tra d·ªØ li·ªáu"""
    conn = psycopg2.connect(Config.get_db_connection_string())
    cursor = conn.cursor()

    cursor.execute("SELECT COUNT(*) FROM poi_locations_uuid;")
    print(f"\nüìä T·ªïng POI: {cursor.fetchone()[0]}")

    cursor.execute(f"""
        SELECT id, name, lat, long, poi_type, avg_star, total_reviews,
               normalize_stars_reviews, ST_AsText(geom)
        FROM poi_locations_uuid
        LIMIT {limit};
    """)

    print(f"\nüìù {limit} records m·∫´u:")
    for r in cursor.fetchall():
        print(f"- {r[1]} | ({r[2]}, {r[3]}) | {r[4]} | geom={r[8]}")

    cursor.close()
    conn.close()


if __name__ == "__main__":
    csv_file = "viamo_full.csv"

    print("üöÄ B·∫ÆT ƒê·∫¶U IMPORT POI")
    print(f"üìÅ File: {csv_file}")
    print(f"üóÑÔ∏è DB: {Config.DB_NAME}@{Config.DB_HOST}\n")

    import_csv_to_postgres(csv_file, batch_size=100)
    verify_data(limit=5)
